# GCD-TFG App: Agente de Presupuestos con LangGraph + Flask + Assistant-UI

## ğŸ“– DescripciÃ³n
Este repositorio contiene un prototipo de un agente de IA para la elaboraciÃ³n de presupuestos de productos, basado en:

- **LangGraph (Python)**: para definir flujos de trabajo (DAGs) que cargan datos estructurados, razonan sobre costes y estiman precios de piezas similares.
- **Flask**: expone el agente mediante una API REST con rutas de presupuestado y streaming (SSE).
- **Assistant-UI (React + Next.js)**: interfaz de chat moderna que consume la API y muestra el razonamiento paso a paso.
- **Ollama / TGI / vLLM**: servidor de modelos LLM de cÃ³digo abierto (localmente) para generar estimaciones.
- **Docker & Docker Compose**: entorno reproducible para levantar el backend, frontend y LLM en contenedores.

## ğŸš€ CaracterÃ­sticas principales

- Carga de ficheros CSV con costes de componentes.
- BÃºsqueda de piezas similares usando heurÃ­sticas o embeddings.
- EstimaciÃ³n de precios mediante prompt a LLM.
- Streaming de pasos de razonamiento a la UI.
- VolÃºmenes para persistir pesos de modelos dentro del proyecto.

## ğŸ› ï¸ Prerrequisitos

- **Docker** (>=24) y **docker-compose**
- **Python 3.11**
- **Node.js 20** y **npm**
- **Git**

## ğŸ“ Estructura del proyecto

```text
GCD-TFG-app/
â”œâ”€â”€ backend/                # API Flask + LangGraph
â”‚   â”œâ”€â”€ api/app.py          # Entrypoint de Flask
â”‚   â”œâ”€â”€ graph/              # DefiniciÃ³n de nodos y flujos LangGraph
â”‚   â”œâ”€â”€ models/             # Pydantic schemas
â”‚   â””â”€â”€ pyproject.toml      # ConfiguraciÃ³n Poetry
â”œâ”€â”€ frontend/ui-budget/     # Assistant-UI template (Next.js)
â”œâ”€â”€ models/ollama/          # Carpeta bind-mount para pesos de LLM
â”œâ”€â”€ ops/                    # Operaciones y scripts de infraestructura
â”‚   â””â”€â”€ docker-compose.dev.yml
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

## âš™ï¸ Entorno de desarrollo

1. Clona el repositorio y crea la carpeta de modelos:
   ```bash
   git clone <repo_url>.git
   cd GCD-TFG-app
   mkdir -p models/ollama
   ```

2. Levanta los contenedores:
   ```bash
   docker-compose -f ops/docker-compose.dev.yml up --build
   ```

3. Comprueba los servicios:
   - **LLM**: disponible en `http://localhost:11434` (podrÃ¡s hacer `ollama pull llama2` dentro del contenedor).
   - **API**: health-check en `http://localhost:8000/health`.
   - **UI**: abre `http://localhost:3000` en tu navegador.

## ğŸ“¥ GestiÃ³n de modelos LLM

Dentro del contenedor `llm` (Ollama):
```bash
# Accede al contenedor
docker-compose exec llm bash
# Descarga un modelo, p.ej:
ollama pull llama2
```
Los pesos se guardarÃ¡n en `models/ollama/` en tu mÃ¡quina.

## ğŸ’¡ Uso bÃ¡sico

1. Desde la UI, envÃ­a un BOM (lista de componentes y cantidades) en el chat.
2. El backend procesarÃ¡ el CSV, ejecutarÃ¡ el grafo LangGraph y mostrarÃ¡ cada paso.
3. ObtendrÃ¡s un desglose de costes y una estimaciÃ³n final.

## ğŸ“ˆ Roadmap de desarrollo

Para mÃ¡s detalles, consulta los _Issues_ y _Projects_ de este repositorio. Principales hitos:

- Fase 1: Entorno dev + Hello World LangGraph.
- Fase 2: Nodos de carga CSV y estimaciÃ³n.
- Fase 3: Streaming SSE y UI React.
- Fase 4: Observabilidad (Prometheus/Grafana).
- Fase 5: Despliegue en staging/producciÃ³n.

## ğŸ“ Contribuciones

Â¡Bienvenidas! Por favor abre Pull Requests y discute las mejoras en los _Issues_.

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. VÃ©ase `LICENSE` para mÃ¡s detalles.